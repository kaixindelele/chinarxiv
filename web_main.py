#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ChinArXivè®ºæ–‡ç¿»è¯‘å™¨ - FastAPI Webåº”ç”¨

ä¸»è¦åŠŸèƒ½ï¼š
1. æ”¯æŒarxivé“¾æ¥/IDè¾“å…¥ç¿»è¯‘
2. æ”¯æŒæœ¬åœ°PDFä¸Šä¼ ç¿»è¯‘
3. å®æ—¶æ—¥å¿—æ›´æ–°ï¼ˆSSEï¼‰
4. ç¼“å­˜ç®¡ç†
5. æ–‡ä»¶ä¸‹è½½

æŠ€æœ¯æ ˆï¼šFastAPI + HTML + JS + CSS
"""

import os
import sys
import json
import asyncio
import hashlib
import shutil
import logging
import requests
import threading
from pathlib import Path
from datetime import datetime
from typing import Optional, Dict, Any, List
from contextlib import asynccontextmanager
from concurrent.futures import ThreadPoolExecutor

from fastapi import FastAPI, UploadFile, File, Form, HTTPException, BackgroundTasks
from fastapi.responses import FileResponse, StreamingResponse, HTMLResponse
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

# å¯¼å…¥ç¿»è¯‘å™¨
from arxiv_translator import ArxivTranslator
from step1_arxiv_downloader import ArxivDownloader
from config import API_KEY, BASE_URL, LLM_MODEL

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# å…¨å±€å˜é‡
CACHE_DIR = Path("./arxiv_cache")
UPLOADS_DIR = Path("./uploads")
STATIC_DIR = Path("./static")
CACHE_METADATA_FILE = CACHE_DIR / "cache_metadata.json"

# åˆ›å»ºå¿…è¦ç›®å½•
CACHE_DIR.mkdir(exist_ok=True)
UPLOADS_DIR.mkdir(exist_ok=True)
STATIC_DIR.mkdir(exist_ok=True)

# ç¿»è¯‘çŠ¶æ€ç®¡ç†
translation_tasks = {}

# çº¿ç¨‹æ± æ‰§è¡Œå™¨
executor = ThreadPoolExecutor(max_workers=3)

class TranslationStatus:
    """ç¿»è¯‘çŠ¶æ€ç®¡ç†"""
    def __init__(self, task_id: str):
        self.task_id = task_id
        self.status = "pending"
        self.progress = 0
        self.logs = []
        self.result_files = []
        self.error = None
        self.start_time = datetime.now()
        self._lock = threading.Lock()  # çº¿ç¨‹å®‰å…¨çš„é”
        
    def add_log(self, message: str):
        """æ·»åŠ æ—¥å¿—ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        log_entry = f"[{timestamp}] {message}"
        with self._lock:
            self.logs.append(log_entry)
        logger.info(message)
        
    def set_progress(self, progress: int):
        """è®¾ç½®è¿›åº¦ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        with self._lock:
            self.progress = progress
            
    def set_status(self, status: str):
        """è®¾ç½®çŠ¶æ€ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        with self._lock:
            self.status = status
            
    def add_result_file(self, file_path: str):
        """æ·»åŠ ç»“æœæ–‡ä»¶ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        with self._lock:
            self.result_files.append(file_path)
            
    def set_error(self, error: str):
        """è®¾ç½®é”™è¯¯ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        with self._lock:
            self.error = error
        
    def to_dict(self):
        """è½¬æ¢ä¸ºå­—å…¸ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        with self._lock:
            return {
                "task_id": self.task_id,
                "status": self.status,
                "progress": self.progress,
                "logs": self.logs.copy(),
                "result_files": self.result_files.copy(),
                "error": self.error,
                "elapsed_time": (datetime.now() - self.start_time).seconds
            }


# ç¼“å­˜ç®¡ç†
class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self):
        self._lock = threading.Lock()  # çº¿ç¨‹å®‰å…¨çš„é”ï¼Œå¿…é¡»å…ˆåˆå§‹åŒ–
        self.metadata = self.load_metadata()
        
    def load_metadata(self) -> Dict[str, Any]:
        """åŠ è½½ç¼“å­˜å…ƒæ•°æ®ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        with self._lock:
            try:
                if CACHE_METADATA_FILE.exists():
                    with open(CACHE_METADATA_FILE, 'r', encoding='utf-8') as f:
                        return json.load(f)
            except Exception as e:
                logger.warning(f"åŠ è½½ç¼“å­˜å…ƒæ•°æ®å¤±è´¥: {e}")
            return {}
    
    def save_metadata(self):
        """ä¿å­˜ç¼“å­˜å…ƒæ•°æ®ï¼ˆçº¿ç¨‹å®‰å…¨ï¼Œå¢é‡ä¿å­˜ï¼‰"""
        with self._lock:
            try:
                # å…ˆè¯»å–æœ€æ–°çš„å…ƒæ•°æ®æ–‡ä»¶
                existing_metadata = {}
                if CACHE_METADATA_FILE.exists():
                    try:
                        with open(CACHE_METADATA_FILE, 'r', encoding='utf-8') as f:
                            existing_metadata = json.load(f)
                    except Exception as e:
                        logger.warning(f"è¯»å–ç°æœ‰å…ƒæ•°æ®å¤±è´¥: {e}")
                
                # åˆå¹¶å½“å‰metadataåˆ°existing_metadataï¼ˆæ–°å¢æ¨¡å¼ï¼‰
                existing_metadata.update(self.metadata)
                
                # å†™å›æ–‡ä»¶
                with open(CACHE_METADATA_FILE, 'w', encoding='utf-8') as f:
                    json.dump(existing_metadata, f, ensure_ascii=False, indent=2)
                
                # æ›´æ–°å†…å­˜ä¸­çš„metadata
                self.metadata = existing_metadata
                
            except Exception as e:
                logger.error(f"ä¿å­˜ç¼“å­˜å…ƒæ•°æ®å¤±è´¥: {e}")
    
    def get_cache_key(self, identifier: str, params: dict) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        cache_content = f"{identifier}|{json.dumps(params, sort_keys=True)}"
        return hashlib.md5(cache_content.encode('utf-8')).hexdigest()
    
    def check_cache(self, identifier: str, params: dict) -> Optional[List[str]]:
        """æ£€æŸ¥ç¼“å­˜ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        cache_key = self.get_cache_key(identifier, params)
        
        with self._lock:
            # å…ˆé‡æ–°åŠ è½½å…ƒæ•°æ®ï¼Œç¡®ä¿è·å–æœ€æ–°æ•°æ®
            if CACHE_METADATA_FILE.exists():
                try:
                    with open(CACHE_METADATA_FILE, 'r', encoding='utf-8') as f:
                        self.metadata = json.load(f)
                except Exception as e:
                    logger.warning(f"é‡æ–°åŠ è½½å…ƒæ•°æ®å¤±è´¥: {e}")
            
            if cache_key in self.metadata:
                cache_info = self.metadata[cache_key]
                files = cache_info.get('files', [])
                
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                existing_files = [f for f in files if Path(f).exists()]
                if existing_files:
                    logger.info(f"æ‰¾åˆ°ç¼“å­˜: {identifier}")
                    return existing_files
        
        return None
    
    def check_local_pdf_cache(self, pdf_path: str, output_bilingual: bool) -> Optional[List[str]]:
        """æ£€æŸ¥æœ¬åœ°PDFç¿»è¯‘ç¼“å­˜"""
        try:
            pdf_file = Path(pdf_path)
            if not pdf_file.exists():
                return None
            
            # æ ¹æ®åŸå§‹æ–‡ä»¶è·¯å¾„ç¡®å®šç¿»è¯‘è¾“å‡ºç›®å½•å’Œæ–‡ä»¶å
            # ä¾‹å¦‚: arxiv_cache/DeepSeek_OCR_paper/DeepSeek_OCR_paper.pdf
            #   -> arxiv_cache/DeepSeek_OCR_paper/translation/DeepSeek_OCR_paper.zh-CN.{dual|mono}.pdf
            
            parent_dir = pdf_file.parent
            translation_dir = parent_dir / "translation"
            basename = pdf_file.stem  # ä¸å«æ‰©å±•åçš„æ–‡ä»¶å
            
            # æ£€æŸ¥ç¿»è¯‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            existing_files = []
            
            # æ£€æŸ¥åŒè¯­ç‰ˆæœ¬
            dual_pdf = translation_dir / f"{basename}.zh-CN.dual.pdf"
            if dual_pdf.exists():
                existing_files.append(str(dual_pdf))
            
            # æ£€æŸ¥å•è¯­ç‰ˆæœ¬
            mono_pdf = translation_dir / f"{basename}.zh-CN.mono.pdf"
            if mono_pdf.exists():
                existing_files.append(str(mono_pdf))
            
            if existing_files:
                logger.info(f"æ‰¾åˆ°æœ¬åœ°PDFç¿»è¯‘ç¼“å­˜: {pdf_path}")
                return existing_files
            
            return None
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥æœ¬åœ°PDFç¼“å­˜å¤±è´¥: {e}")
            return None
    
    def add_local_pdf_cache(self, pdf_path: str, files: List[str], user_requirements: str = ""):
        """æ·»åŠ æœ¬åœ°PDFç¿»è¯‘åˆ°ç¼“å­˜ï¼ˆçº¿ç¨‹å®‰å…¨ï¼Œå¢é‡æ¨¡å¼ï¼‰"""
        try:
            # ä½¿ç”¨åŸå§‹PDFè·¯å¾„ä½œä¸ºæ ‡è¯†ç¬¦
            pdf_file = Path(pdf_path)
            identifier = str(pdf_file.relative_to(CACHE_DIR)) if pdf_file.is_relative_to(CACHE_DIR) else str(pdf_file)
            
            # ç”Ÿæˆç¼“å­˜é”®
            cache_params = {
                "user_requirements": user_requirements,
                "output_bilingual": True,
                "type": "local_pdf"
            }
            cache_key = self.get_cache_key(identifier, cache_params)
            
            total_size = sum(Path(f).stat().st_size for f in files if Path(f).exists())
            
            with self._lock:
                # å…ˆé‡æ–°åŠ è½½æœ€æ–°å…ƒæ•°æ®
                if CACHE_METADATA_FILE.exists():
                    try:
                        with open(CACHE_METADATA_FILE, 'r', encoding='utf-8') as f:
                            self.metadata = json.load(f)
                    except Exception as e:
                        logger.warning(f"é‡æ–°åŠ è½½å…ƒæ•°æ®å¤±è´¥: {e}")
                
                # æ–°å¢ç¼“å­˜æ¡ç›®ï¼ˆarxiv_idç­‰å­—æ®µç½®ç©ºï¼‰
                self.metadata[cache_key] = {
                    'arxiv_id': "",  # æœ¬åœ°PDFæ²¡æœ‰arxiv_id
                    'arxiv_input': "",  # æœ¬åœ°PDFæ²¡æœ‰arxivè¾“å…¥
                    'user_requirements': user_requirements,
                    'user_terms': "",
                    'identifier': identifier,
                    'file_path': files[0] if files else "",  # ä¸»è¦æ–‡ä»¶è·¯å¾„
                    'original_path': pdf_path,  # åŸå§‹PDFè·¯å¾„
                    'files': files,  # æ‰€æœ‰ç¿»è¯‘æ–‡ä»¶
                    'created_time': datetime.now().isoformat(),
                    'total_size': total_size,
                    'type': 'local_pdf'
                }
            
            # ä¿å­˜æ—¶ä¼šè‡ªåŠ¨åˆå¹¶
            self.save_metadata()
            logger.info(f"æœ¬åœ°PDFç¿»è¯‘å·²ç¼“å­˜: {identifier}")
            
        except Exception as e:
            logger.error(f"æ·»åŠ æœ¬åœ°PDFç¼“å­˜å¤±è´¥: {e}")
    
    def add_cache(self, identifier: str, params: dict, files: List[str]):
        """æ·»åŠ arxivç¼“å­˜ï¼ˆçº¿ç¨‹å®‰å…¨ï¼Œå¢é‡æ¨¡å¼ï¼‰"""
        cache_key = self.get_cache_key(identifier, params)
        
        total_size = sum(Path(f).stat().st_size for f in files if Path(f).exists())
        
        with self._lock:
            # å…ˆé‡æ–°åŠ è½½æœ€æ–°å…ƒæ•°æ®
            if CACHE_METADATA_FILE.exists():
                try:
                    with open(CACHE_METADATA_FILE, 'r', encoding='utf-8') as f:
                        self.metadata = json.load(f)
                except Exception as e:
                    logger.warning(f"é‡æ–°åŠ è½½å…ƒæ•°æ®å¤±è´¥: {e}")
            
            # æ–°å¢ç¼“å­˜æ¡ç›®
            self.metadata[cache_key] = {
                'arxiv_id': identifier,
                'arxiv_input': identifier,
                'user_requirements': params.get('user_requirements', ''),
                'user_terms': params.get('user_terms', ''),
                'identifier': identifier,
                'file_path': files[0] if files else "",
                'original_path': files[0] if files else "",
                'files': files,
                'created_time': datetime.now().isoformat(),
                'total_size': total_size,
                'type': 'arxiv'
            }
        
        # ä¿å­˜æ—¶ä¼šè‡ªåŠ¨åˆå¹¶
        self.save_metadata()
        logger.info(f"Arxivç¼“å­˜å·²æ·»åŠ : {identifier}")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ï¼ˆçº¿ç¨‹å®‰å…¨ï¼Œåˆ·æ–°æœ€æ–°æ•°æ®ï¼‰"""
        with self._lock:
            # é‡æ–°åŠ è½½æœ€æ–°å…ƒæ•°æ®
            if CACHE_METADATA_FILE.exists():
                try:
                    with open(CACHE_METADATA_FILE, 'r', encoding='utf-8') as f:
                        self.metadata = json.load(f)
                except Exception as e:
                    logger.warning(f"é‡æ–°åŠ è½½å…ƒæ•°æ®å¤±è´¥: {e}")
            
            total_count = len(self.metadata)
            total_size = sum(info.get('total_size', 0) for info in self.metadata.values())
            
            return {
                'count': total_count,
                'size_mb': total_size / (1024 * 1024)
            }
    
    def clear_cache(self) -> int:
        """æ¸…ç†ç¼“å­˜ï¼ˆå·²ç¦ç”¨ - ä¸å†æ¸…ç©ºç¼“å­˜ï¼Œä»…ç”¨äºæœªæ¥æ‰©å±•ï¼‰"""
        logger.warning("clear_cacheå·²ç¦ç”¨ï¼Œä¸ä¼šæ¸…ç©ºä»»ä½•ç¼“å­˜æ•°æ®")
        return 0


# åˆ›å»ºå…¨å±€ç¼“å­˜ç®¡ç†å™¨
cache_manager = CacheManager()


# FastAPIåº”ç”¨
@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    logger.info("å¯åŠ¨ChinArXivç¿»è¯‘å™¨...")
    yield
    logger.info("å…³é—­ChinArXivç¿»è¯‘å™¨...")

app = FastAPI(
    title="ChinArXivè®ºæ–‡ç¿»è¯‘å™¨",
    description="æ”¯æŒarxivè®ºæ–‡å’Œæœ¬åœ°PDFç¿»è¯‘",
    version="1.0.0",
    lifespan=lifespan
)

# CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ============= è·¯ç”±å¤„ç† =============

@app.get("/", response_class=HTMLResponse)
async def index():
    """ä¸»é¡µ"""
    index_file = STATIC_DIR / "index.html"
    if index_file.exists():
        return FileResponse(index_file)
    else:
        return HTMLResponse("<h1>è¯·åˆ›å»º static/index.html æ–‡ä»¶</h1>")


@app.get("/api/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "ok",
        "timestamp": datetime.now().isoformat()
    }


@app.get("/api/cache/stats")
async def get_cache_stats():
    """è·å–ç¼“å­˜ç»Ÿè®¡"""
    stats = cache_manager.get_cache_stats()
    return {
        "success": True,
        "stats": stats
    }


@app.post("/api/cache/clear")
async def clear_cache():
    """æ¸…ç†ç¼“å­˜"""
    deleted_count = cache_manager.clear_cache()
    return {
        "success": True,
        "message": f"å·²æ¸…ç† {deleted_count} ä¸ªç¼“å­˜æ–‡ä»¶"
    }


@app.post("/api/translate/arxiv")
async def translate_arxiv(
    arxiv_input: str = Form(...),
    user_requirements: str = Form("ä¿æŒå­¦æœ¯æ€§å’Œä¸“ä¸šæ€§ï¼Œç¡®ä¿æœ¯è¯­ç¿»è¯‘çš„ä¸€è‡´æ€§"),
    user_terms: str = Form(""),
    output_bilingual: bool = Form(False),
    force_retranslate: bool = Form(False),
    background_tasks: BackgroundTasks = None
):
    """
    ç¿»è¯‘arxivè®ºæ–‡
    
    æµç¨‹ï¼š
    1. æ£€æŸ¥ç¼“å­˜ï¼ˆå¦‚æœä¸å¼ºåˆ¶é‡æ–°ç¿»è¯‘ï¼‰
    2. å°è¯•ArxivTranslatorï¼ˆlatexç¿»è¯‘ï¼‰
    3. å¤±è´¥åˆ™ä¸‹è½½PDFåˆ° arxiv_cache/arxiv_id/extract/
    4. ç”¨babeldocç¿»è¯‘ï¼Œè¾“å‡ºåˆ° arxiv_cache/arxiv_id/translation/
    """
    task_id = hashlib.md5(f"{arxiv_input}{datetime.now()}".encode()).hexdigest()[:8]
    
    # åˆ›å»ºç¿»è¯‘çŠ¶æ€
    status = TranslationStatus(task_id)
    translation_tasks[task_id] = status
    
    # åœ¨çº¿ç¨‹æ± ä¸­å¯åŠ¨ç¿»è¯‘ä»»åŠ¡
    executor.submit(
        translate_arxiv_task_sync,
        task_id,
        arxiv_input,
        user_requirements,
        user_terms,
        output_bilingual,
        force_retranslate
    )
    
    return {
        "success": True,
        "task_id": task_id,
        "message": "ç¿»è¯‘ä»»åŠ¡å·²å¯åŠ¨"
    }


@app.post("/api/translate/upload")
async def translate_upload(
    file: UploadFile = File(...),
    user_requirements: str = Form("ä¿æŒå­¦æœ¯æ€§å’Œä¸“ä¸šæ€§ï¼Œç¡®ä¿æœ¯è¯­ç¿»è¯‘çš„ä¸€è‡´æ€§"),
    output_bilingual: bool = Form(False),
    background_tasks: BackgroundTasks = None
):
    """
    ç¿»è¯‘ä¸Šä¼ çš„PDF
    
    æµç¨‹ï¼š
    1. ä¿å­˜åˆ° arxiv_cache/filename/
    2. ç”¨babeldocç¿»è¯‘ï¼Œè¾“å‡ºåˆ° arxiv_cache/filename/translation/
    """
    if not file.filename.endswith('.pdf'):
        raise HTTPException(status_code=400, detail="åªæ”¯æŒPDFæ–‡ä»¶")
    
    task_id = hashlib.md5(f"{file.filename}{datetime.now()}".encode()).hexdigest()[:8]
    
    # åˆ›å»ºç¿»è¯‘çŠ¶æ€
    status = TranslationStatus(task_id)
    translation_tasks[task_id] = status
    
    # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
    filename = Path(file.filename).stem
    upload_dir = CACHE_DIR / filename
    upload_dir.mkdir(exist_ok=True)
    
    file_path = upload_dir / file.filename
    
    try:
        with open(file_path, "wb") as f:
            content = await file.read()
            f.write(content)
        
        status.add_log(f"æ–‡ä»¶å·²ä¿å­˜: {file_path}")
        
    except Exception as e:
        status.status = "error"
        status.error = f"æ–‡ä»¶ä¿å­˜å¤±è´¥: {e}"
        return {"success": False, "error": str(e)}
    
    # åœ¨çº¿ç¨‹æ± ä¸­å¯åŠ¨ç¿»è¯‘ä»»åŠ¡
    executor.submit(
        translate_upload_task_sync,
        task_id,
        str(file_path),
        filename,
        user_requirements,
        output_bilingual
    )
    
    return {
        "success": True,
        "task_id": task_id,
        "message": "ä¸Šä¼ æˆåŠŸï¼Œç¿»è¯‘ä»»åŠ¡å·²å¯åŠ¨"
    }


@app.get("/api/translate/status/{task_id}")
async def get_translation_status(task_id: str):
    """è·å–ç¿»è¯‘çŠ¶æ€"""
    if task_id not in translation_tasks:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    status = translation_tasks[task_id]
    return {
        "success": True,
        "status": status.to_dict()
    }


@app.get("/api/translate/logs/{task_id}")
async def stream_logs(task_id: str):
    """å®æ—¶æ—¥å¿—æµï¼ˆSSEï¼‰"""
    if task_id not in translation_tasks:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    async def generate():
        status = translation_tasks[task_id]
        last_log_index = 0
        
        while True:
            try:
                # è·å–æ–°æ—¥å¿—ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
                with status._lock:
                    current_logs = status.logs[last_log_index:]
                    current_progress = status.progress
                    current_status = status.status
                    current_files = status.result_files.copy()
                    current_error = status.error
                
                # å‘é€æ–°æ—¥å¿—
                for log in current_logs:
                    yield f"data: {json.dumps({'type': 'log', 'message': log}, ensure_ascii=False)}\n\n"
                
                last_log_index += len(current_logs)
                
                # å‘é€è¿›åº¦æ›´æ–°
                yield f"data: {json.dumps({'type': 'progress', 'progress': current_progress, 'status': current_status}, ensure_ascii=False)}\n\n"
                
                # å¦‚æœå®Œæˆæˆ–å¤±è´¥ï¼Œå‘é€æœ€ç»ˆæ¶ˆæ¯
                if current_status in ["completed", "error"]:
                    if current_status == "completed":
                        yield f"data: {json.dumps({'type': 'success', 'files': current_files}, ensure_ascii=False)}\n\n"
                    else:
                        yield f"data: {json.dumps({'type': 'error', 'message': current_error}, ensure_ascii=False)}\n\n"
                    
                    yield f"data: {json.dumps({'type': 'done'})}\n\n"
                    break
                
                await asyncio.sleep(0.3)  # æ›´é¢‘ç¹çš„æ›´æ–°
                
            except Exception as e:
                logger.error(f"æ—¥å¿—æµé”™è¯¯: {e}")
                break
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        }
    )


@app.get("/api/download/{task_id}/{filename}")
async def download_file(task_id: str, filename: str):
    """ä¸‹è½½ç¿»è¯‘ç»“æœ"""
    if task_id not in translation_tasks:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    status = translation_tasks[task_id]
    
    # æŸ¥æ‰¾æ–‡ä»¶
    for file_path in status.result_files:
        if Path(file_path).name == filename:
            if Path(file_path).exists():
                return FileResponse(
                    file_path,
                    filename=filename,
                    media_type="application/pdf"
                )
    
    raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")


# ============= åå°ç¿»è¯‘ä»»åŠ¡ =============

def translate_arxiv_task_sync(
    task_id: str,
    arxiv_input: str,
    user_requirements: str,
    user_terms: str,
    output_bilingual: bool,
    force_retranslate: bool
):
    """arxivç¿»è¯‘ä»»åŠ¡ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼Œåœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œï¼‰"""
    status = translation_tasks[task_id]
    
    try:
        status.set_status("running")
        status.add_log("å¼€å§‹ç¿»è¯‘arxivè®ºæ–‡...")
        status.set_progress(5)
        
        # è§£æarxiv ID
        downloader = ArxivDownloader()
        success_parse, arxiv_id, _ = downloader.parse_arxiv_input(arxiv_input)
        
        if not success_parse:
            raise ValueError("æ— æ³•è§£æarxivè¾“å…¥")
        
        status.add_log(f"arxiv ID: {arxiv_id}")
        status.set_progress(10)
        
        # æ£€æŸ¥ç¼“å­˜
        cache_params = {
            "user_requirements": user_requirements,
            "user_terms": user_terms,
            "output_bilingual": output_bilingual
        }
        
        if not force_retranslate:
            cached_files = cache_manager.check_cache(arxiv_id, cache_params)
            if cached_files:
                status.add_log("ä½¿ç”¨ç¼“å­˜ç»“æœ")
                for f in cached_files:
                    status.add_result_file(f)
                status.set_status("completed")
                status.set_progress(100)
                return
        
        status.add_log("å°è¯•ä½¿ç”¨ArxivTranslatorç¿»è¯‘...")
        status.set_progress(20)
        
        # å°è¯•ArxivTranslatorç¿»è¯‘
        arxiv_dir = CACHE_DIR / arxiv_id
        translation_dir = arxiv_dir / "translation"
        translation_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–ç¿»è¯‘å™¨
        translator = ArxivTranslator(
            cache_dir=str(CACHE_DIR),
            output_dir=str(translation_dir),
            work_dir=str(translation_dir),
            api_key=API_KEY,
            base_url=BASE_URL,
            llm_model=LLM_MODEL
        )
        
        # è§£æç”¨æˆ·æœ¯è¯­
        user_terms_dict = {}
        if user_terms.strip():
            for line in user_terms.strip().split('\n'):
                if ':' in line or 'ï¼š' in line:
                    separator = ':' if ':' in line else 'ï¼š'
                    key, value = line.split(separator, 1)
                    user_terms_dict[key.strip()] = value.strip()
        
        # è¿›åº¦å›è°ƒ
        def progress_callback(step, prog, message):
            status.set_progress(20 + int(prog * 0.5))  # 20-70%
            status.add_log(f"Step {step}: {message}")
        
        # æ‰§è¡Œç¿»è¯‘
        success, result, details = translator.translate_arxiv(
            arxiv_input=arxiv_input,
            user_requirements=user_requirements,
            user_terms=user_terms_dict,
            progress_callback=progress_callback,
            compile_pdf=True
        )
        
        if success:
            # æ£€æŸ¥è¿”å›çš„æ˜¯PDFè¿˜æ˜¯TEXæ–‡ä»¶
            result_path = Path(result)
            if result_path.suffix.lower() == '.pdf':
                # è¿”å›çš„æ˜¯PDFæ–‡ä»¶ï¼Œç¿»è¯‘æˆåŠŸ
                status.add_log("ArxivTranslatorç¿»è¯‘æˆåŠŸï¼")
                status.add_result_file(result)
                status.set_progress(100)
                status.set_status("completed")
                
                # æ·»åŠ åˆ°ç¼“å­˜
                cache_manager.add_cache(arxiv_id, cache_params, [result])
                return
            elif result_path.suffix.lower() == '.tex':
                # è¿”å›çš„æ˜¯TEXæ–‡ä»¶ï¼Œè¯´æ˜PDFç¼–è¯‘å¤±è´¥ï¼Œéœ€è¦ä½¿ç”¨babeldoc
                status.add_log(f"ArxivTranslatorè¿”å›texæ–‡ä»¶: {result_path.name}")
                status.add_log("PDFç¼–è¯‘å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨babeldocç¿»è¯‘...")
                status.set_progress(70)
                # ç»§ç»­æ‰§è¡Œä¸‹é¢çš„babeldocç¿»è¯‘æµç¨‹
            else:
                # æœªçŸ¥æ–‡ä»¶ç±»å‹
                status.add_log(f"è­¦å‘Š: æœªçŸ¥çš„è¿”å›æ–‡ä»¶ç±»å‹: {result_path.suffix}")
                status.add_result_file(result)
                status.set_progress(100)
                status.set_status("completed")
                cache_manager.add_cache(arxiv_id, cache_params, [result])
                return
        else:
            # ArxivTranslatorå¤±è´¥ï¼Œä½¿ç”¨babeldoc
            status.add_log("ArxivTranslatorç¿»è¯‘å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨babeldoc...")
            status.set_progress(70)
        
        # ä¸‹è½½PDFåˆ°extractç›®å½•
        extract_dir = arxiv_dir / "extract"
        extract_dir.mkdir(parents=True, exist_ok=True)
        
        pdf_path = extract_dir / f"{arxiv_id}.pdf"
        
        if not pdf_path.exists():
            status.add_log("ä¸‹è½½arxiv PDF...")
            pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"
            
            response = requests.get(pdf_url, timeout=60)
            if response.status_code == 200:
                with open(pdf_path, 'wb') as f:
                    f.write(response.content)
                status.add_log(f"PDFå·²ä¸‹è½½: {pdf_path}")
            else:
                raise Exception(f"ä¸‹è½½PDFå¤±è´¥: HTTP {response.status_code}")
        
        status.set_progress(80)
        
        # ä½¿ç”¨babeldocç¿»è¯‘
        status.add_log("ä½¿ç”¨babeldocç¿»è¯‘PDF...")
        result_files = translate_with_babeldoc_sync(
            status,
            str(pdf_path),
            str(translation_dir),
            output_bilingual
        )
        
        if result_files:
            for f in result_files:
                status.add_result_file(f)
            status.set_status("completed")
            status.set_progress(100)
            status.add_log("ç¿»è¯‘å®Œæˆï¼")
            
            # æ·»åŠ åˆ°ç¼“å­˜
            cache_manager.add_cache(arxiv_id, cache_params, result_files)
        else:
            raise Exception("babeldocç¿»è¯‘å¤±è´¥")
        
    except Exception as e:
        status.set_status("error")
        status.set_error(str(e))
        status.add_log(f"é”™è¯¯: {e}")
        logger.error(f"ç¿»è¯‘å¤±è´¥: {e}", exc_info=True)


def translate_upload_task_sync(
    task_id: str,
    pdf_path: str,
    filename: str,
    user_requirements: str,
    output_bilingual: bool
):
    """ä¸Šä¼ æ–‡ä»¶ç¿»è¯‘ä»»åŠ¡ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼Œåœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œï¼‰"""
    status = translation_tasks[task_id]
    
    try:
        status.set_status("running")
        status.add_log("å¼€å§‹ç¿»è¯‘ä¸Šä¼ çš„PDF...")
        status.set_progress(5)
        
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç¿»è¯‘ç¼“å­˜
        status.add_log("æ£€æŸ¥ç¿»è¯‘ç¼“å­˜...")
        cached_files = cache_manager.check_local_pdf_cache(pdf_path, output_bilingual)
        
        if cached_files:
            status.add_log(f"æ‰¾åˆ°å·²ç¿»è¯‘çš„æ–‡ä»¶ï¼Œä½¿ç”¨ç¼“å­˜ç»“æœ")
            for f in cached_files:
                status.add_result_file(f)
                status.add_log(f"ç¼“å­˜æ–‡ä»¶: {Path(f).name}")
            status.set_status("completed")
            status.set_progress(100)
            status.add_log("ç¿»è¯‘å®Œæˆï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰ï¼")
            return
        
        status.add_log("æœªæ‰¾åˆ°ç¼“å­˜ï¼Œå¼€å§‹æ–°çš„ç¿»è¯‘ä»»åŠ¡")
        status.set_progress(10)
        
        # è¾“å‡ºç›®å½•
        output_dir = CACHE_DIR / filename / "translation"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        status.add_log(f"è¾“å‡ºç›®å½•: {output_dir}")
        status.set_progress(20)
        
        # ä½¿ç”¨babeldocç¿»è¯‘
        status.add_log("ä½¿ç”¨babeldocç¿»è¯‘PDF...")
        result_files = translate_with_babeldoc_sync(
            status,
            pdf_path,
            str(output_dir),
            output_bilingual
        )
        
        if result_files:
            for f in result_files:
                status.add_result_file(f)
            status.set_status("completed")
            status.set_progress(100)
            status.add_log("ç¿»è¯‘å®Œæˆï¼")
            
            # æ·»åŠ åˆ°ç¼“å­˜
            status.add_log("ä¿å­˜ç¿»è¯‘ç»“æœåˆ°ç¼“å­˜...")
            cache_manager.add_local_pdf_cache(pdf_path, result_files, user_requirements)
            status.add_log("ç¼“å­˜å·²æ›´æ–°")
        else:
            raise Exception("babeldocç¿»è¯‘å¤±è´¥")
        
    except Exception as e:
        status.set_status("error")
        status.set_error(str(e))
        status.add_log(f"é”™è¯¯: {e}")
        logger.error(f"ç¿»è¯‘å¤±è´¥: {e}", exc_info=True)


def translate_with_babeldoc_sync(
    status: TranslationStatus,
    pdf_path: str,
    output_dir: str,
    no_dual: bool = True
) -> List[str]:
    """ä½¿ç”¨babeldocç¿»è¯‘PDFï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
    try:
        url = "http://localhost:8321/translate/stream"
        
        payload = {
            "pdf_path": pdf_path,
            "output_dir": output_dir,
            "no_dual": no_dual
        }
        
        status.add_log("è¿æ¥babeldocæœåŠ¡...")
        
        # å‘é€è¯·æ±‚
        response = requests.post(url, json=payload, stream=True, timeout=3600,
                                 proxies={'http': None, 'https': None})
        
        if response.status_code != 200:
            status.add_log(f"babeldocæœåŠ¡é”™è¯¯: HTTP {response.status_code}")
            return []
        
        pdf_files = []
        
        # è·å–å½“å‰è¿›åº¦
        with status._lock:
            base_progress = status.progress
        
        # é€è¡Œè¯»å–SSEå“åº”
        for line in response.iter_lines():
            if not line:
                continue
            
            line_str = line.decode('utf-8')
            
            if line_str.startswith("data: "):
                data_str = line_str[6:]
                
                try:
                    data = json.loads(data_str)
                    msg_type = data.get("type", "unknown")
                    
                    if msg_type == "log":
                        status.add_log(data.get("message", ""))
                    
                    elif msg_type == "success":
                        pdf_files = data.get("pdf_paths", [])
                        status.add_log(f"ç”Ÿæˆäº† {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")
                    
                    elif msg_type == "error":
                        status.add_log(f"é”™è¯¯: {data.get('message', 'æœªçŸ¥é”™è¯¯')}")
                    
                    elif msg_type == "done":
                        status.add_log("babeldocç¿»è¯‘å®Œæˆ")
                        break
                    
                    # æ›´æ–°è¿›åº¦
                    status.set_progress(min(base_progress + 20, 95))
                    
                except json.JSONDecodeError:
                    pass
        
        return pdf_files
        
    except Exception as e:
        status.add_log(f"babeldocç¿»è¯‘å¼‚å¸¸: {e}")
        logger.error(f"babeldocç¿»è¯‘å¼‚å¸¸: {e}", exc_info=True)
        return []


# ============= é™æ€æ–‡ä»¶æœåŠ¡ =============

# æŒ‚è½½é™æ€æ–‡ä»¶ç›®å½•
app.mount("/static", StaticFiles(directory=str(STATIC_DIR)), name="static")


if __name__ == "__main__":
    import uvicorn
    
    print("=" * 70)
    print("ğŸŒ ChinArXivè®ºæ–‡ç¿»è¯‘å™¨")
    print("=" * 70)
    print(f"ğŸ“Š é…ç½®ä¿¡æ¯:")
    print(f"   LLMæ¨¡å‹: {LLM_MODEL}")
    print(f"   APIåœ°å€: {BASE_URL}")
    print(f"   ç¼“å­˜ç›®å½•: {CACHE_DIR}")
    print(f"\nğŸš€ å¯åŠ¨WebæœåŠ¡...")
    print(f"   è®¿é—®åœ°å€: http://localhost:12985")
    print("=" * 70)
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=12985,
        log_level="info"
    )

